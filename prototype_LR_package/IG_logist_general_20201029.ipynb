{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing missing packages:\n",
      "{'harmonypy'}\n",
      "scanpy==1.4.5.1 anndata==0.7.1 umap==0.3.10 numpy==1.18.1 scipy==1.4.1 pandas==1.0.1 scikit-learn==0.22.2 statsmodels==0.11.1 python-igraph==0.8.0 louvain==0.6.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'bbknn', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing = required - installed\n",
    "if missing:\n",
    "    print(\"Installing missing packages:\" )\n",
    "    print(missing)\n",
    "    python = sys.executable\n",
    "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from bbknn import bbknn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "import harmonypy\n",
    "from pathlib import Path\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.settings.set_figure_params(dpi=80, color_map='viridis')\n",
    "sc.logging.print_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the hybrid dataset containing categories of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below block is the only variable assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce variables\n",
    "\n",
    "# Introduce the path you'd like to save data to\n",
    "save_data_name = \"./example.h5ad\"\n",
    "\n",
    "# Name of first object\n",
    "data1 = \"_YS_train\"\n",
    "# Provide path to obj1 // landscape/training data\n",
    "Object1 = \"../projects/YS/YS_data/matched_liver_data/1809220_matched_liv_progen_update.h5ad\"\n",
    "# Provide categorical to join between datasets\n",
    "cat1 = \"160920_cell_labels\"\n",
    "\n",
    "# Name of second object\n",
    "data2 = \"_YS_predicted\"\n",
    "# Provide path to obj2 // prediction/projection data\n",
    "Object2 = \"../projects/YS/YS_data/matched_liver_data/1809220_matched_liv_progen_update.h5ad\"\n",
    "# Provide categorical to join between datasets\n",
    "cat2 = \"160920_cell_labels\"\n",
    "\n",
    "# Misc Options\n",
    "subsample_train = False #samples the training data to the smallest fraction (highly dependent on resolution of input categorical)\n",
    "batch_correction = \"Harmony\" # Will accept Harmony, BBKNN or False as options\n",
    "train_x = 'X_pca' # Define the resource to train and predict on, PCA, X or UMAP (#if you wish to use gene expression, train_x = 'X')\n",
    "remove_effect_of_custom_gene_list = 'NA' #\"./cell_cycle_genes.csv\" #remove a custom list of genes from just the variable genes to compute PCA from. Your .csv should have HGNC gene names in the first column to be read in as a vector, any column name is fine.\n",
    "\n",
    "# LR options\n",
    "penalty='l2' \n",
    "sparcity=0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine and pre-process data to match correlations across PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filepaths are good\n",
    "if (Path(Object1).is_file() & Path(Object2).is_file()):\n",
    "    print(\"adata file paths detetcted, proceeding to load\")\n",
    "    adata = sc.read(Object1)\n",
    "    adata2 =  sc.read(Object2)\n",
    "else: \n",
    "    raise TypeError(\"one or more .h5ad paths cannot be accessed\")\n",
    "\n",
    "# Combine and pre-process data to match correlations across PCA\n",
    "\n",
    "# Define intersecting genes between datasets\n",
    "adata_genes = list(adata.var.index)\n",
    "adata2_genes = list(adata2.var.index)\n",
    "keep_SC_genes = list(set(adata_genes) & set(adata2_genes))\n",
    "print(\"keep gene list = \" , len(keep_SC_genes), \"adata1 gene length = \", len(adata_genes) , \"adata2 gene length = \", len(adata2_genes) )\n",
    "\n",
    "# Remove non-intersecting genes (this step will remove cite-seq data if training data is pure RNA seq)\n",
    "adata_intersect1 = adata[:, keep_SC_genes]\n",
    "adata = adata_intersect1\n",
    "adata_intersect2 = adata2[:, keep_SC_genes]\n",
    "adata2 = adata_intersect2\n",
    "\n",
    "#optional subsampling of training data to \n",
    "if(subsample_train == True):\n",
    "    data = adata.obs\n",
    "    data = data.sample(frac=1).groupby('corr_concat').head(min(adata.obs.groupby(cat1).size()))\n",
    "    keep = data.index\n",
    "    adata = adata[adata.obs.index.isin(keep)]\n",
    "\n",
    "# Create a common obs column in both datasets containing the data origin tag\n",
    "common_cat = \"corr_concat\" \n",
    "adata.obs[common_cat] = adata.obs[cat1].astype(str) + data1\n",
    "adata2.obs[common_cat] = adata2.obs[cat2].astype(str) + data2\n",
    "adata.obs = adata.obs.astype('category')\n",
    "adata2.obs = adata2.obs.astype('category')\n",
    "concat = adata2.concatenate(adata, join='inner',index_unique=None, batch_categories=None)\n",
    "adata = concat[:]\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.1, max_mean=4)\n",
    "sc.pp.scale(adata, zero_center=True, max_value=None, copy=False)\n",
    "\n",
    "# Optionally remove genes of known confounding effect from variable list\n",
    "if not (Path(remove_effect_of_custom_gene_list).is_file()):\n",
    "    print(\"Custom gene list option is not selected or path is not readbale, proceeding with no variable removal\")\n",
    "else: \n",
    "    print(\"Custom gene removal list detected, proceeding to remove intersect from variable genes\")\n",
    "    regress_list = pd.read_csv(remove_effect_of_custom_gene_list)\n",
    "    regress_list = regress_list.iloc[:, 0]\n",
    "    adata.var[\"highly_variable\"][adata.var.index.isin(regress_list)] = \"False\"\n",
    "    \n",
    "#Now compute PCA\n",
    "sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack')\n",
    "\n",
    "#batch correction options\n",
    "if(batch_correction == \"Harmony\"):\n",
    "    print(\"Commencing harmony\")\n",
    "    # Create hm subset\n",
    "    adata_hm = adata[:]\n",
    "    # Set harmony variables\n",
    "    data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "    meta_data = adata_hm.obs\n",
    "    vars_use = [batch]\n",
    "    # Run Harmony\n",
    "    res = (pd.DataFrame(ho.Z_corr)).T\n",
    "    res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "    # Insert coordinates back into object\n",
    "    adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "    adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "    # Run neighbours\n",
    "    sc.pp.neighbors(adata_hm,n_neighbors=15, n_pcs=50)\n",
    "    adata = adata_hm[:]\n",
    "elif(batch_correction == \"BBKNN\"):\n",
    "    print(\"Commencing BBKNN\")\n",
    "    sc.external.pp.bbknn(adata, batch_key='fetal_ids', approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function require compute power, will take a while \n",
    "\n",
    "def LR_compare(adata, train_x, train_label, subset_predict, subset_train, penalty=penalty, sparcity=sparcity, \n",
    "               col_name='predicted'):\n",
    "\n",
    "    # adata - training+prediction adata object (combined). Pre-processed already\n",
    "    # sparsity - larger sparsity, more bins, more conservative predictions, less accurate. Low sparist for clean output\n",
    "                # A value of 0.2 is reasonable for L2 ridge regression\n",
    "    # penalty - acts as buffer for assigning bins too harshly\n",
    "    # train_x - arg refers to where you would like to derive your training reference from, i.e., GEX (X) or/elif.\n",
    "                # PCA/UMAP in obsm. The two 'if' statements below handle train_x differently based on this\n",
    "                # Based on train_x, the loops below compute 'train_label' (cell type values in training/landscape data) \n",
    "                # and 'predict_x'(prediction data equivalent of train_x)\n",
    "    # train_label - cell type values in training/landscape data\n",
    "    # subset_predict - mandatory subset of predict_x which contains metadata for expression\n",
    "    # subset_train - mandatory subset of train_x which contains metadata for expression\n",
    "    \n",
    "    # Redefine LR parameters 'penalty' and 'sparsity' if you would like to deviate from defaults set above\n",
    "    \n",
    "    # Assign 'lr' as sklearn logistic regression func, with penalty and sparsity defined above\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity)\n",
    "\n",
    "    if train_x == 'X':\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[common_cat].values\n",
    "        train_label = train_label[subset_train]\n",
    "        train_x = adata.X,\n",
    "        # Define prediction parameters\n",
    "        predict_x = train_x\n",
    "        train_x = train_x[subset_train, :]\n",
    "        predict_x = train_x\n",
    "        predict_x = predict_x[subset_predict]\n",
    "\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[common_cat].values\n",
    "        train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "        predict_x = train_x\n",
    "        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "        predict_x = predict_x[subset_predict]\n",
    "        predict_x = pd.DataFrame(predict_x)\n",
    "        predict_x.index = adata.obs[subset_predict].index\n",
    "        \n",
    "    #Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    lr.fit(train_x, train_label)\n",
    "    predict = lr.predict_proba(predict_x)\n",
    "\n",
    "    # Create prediction table and map to adata.obs (in adata.obs[\"predict\"] in the combined object), for the cells that\n",
    "    # are in predict dataset\n",
    "    predict = lr.predict(predict_x)\n",
    "    predict = pd.DataFrame(predict)\n",
    "    predict.index = adata.obs[subset_predict].index\n",
    "    adata.obs[col_name] = adata.obs.index\n",
    "    adata.obs[col_name] = adata.obs[col_name].map(predict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load function for viewing probability by html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot probability table by html\n",
    "def cross_table(adata, x, y, normalise=None, highlight=False, subset=None):                                                                                                                                                                                              \n",
    "    \"\"\"Make a cross table comparing two categorical annotations\n",
    "    \"\"\"\n",
    "    x_attr = adata.obs[x]\n",
    "    y_attr = adata.obs[y]\n",
    "    if subset is not None:\n",
    "        x_attr = x_attr[subset]\n",
    "        y_attr = y_attr[subset]\n",
    "    crs_tbl = pd.crosstab(x_attr, y_attr)\n",
    "    if normalise == 'x':\n",
    "        x_sizes = x_attr.groupby(x_attr).size().values\n",
    "        crs_tbl = (crs_tbl.T / x_sizes).round(2).T\n",
    "    elif normalise == 'y':\n",
    "        y_sizes = x_attr.groupby(y_attr).size().values\n",
    "        crs_tbl = (crs_tbl / y_sizes).round(2)\n",
    "    if highlight:\n",
    "        return crs_tbl.style.background_gradient(cmap='viridis', axis=0)\n",
    "    return crs_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The magic happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression pre-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the separator category in the column of interest, this works by partial matches and enables a-symmetric \n",
    "# comparisons\n",
    "Data1_group = data1\n",
    "Data2_group = data2\n",
    "# Define the common .obs column between concatinated data\n",
    "common_cat = \"corr_concat\"\n",
    "\n",
    "# This block defines subset_predict and subset_train and also runs LR_compare function\n",
    "group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "group1 = list(group1)\n",
    "group2 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data2_group)]).unique()\n",
    "group2 = list(group2)\n",
    "subset_predict = np.array(adata.obs[common_cat].isin(group2))\n",
    "subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "train_label = (adata.obs[common_cat][adata.obs[common_cat].isin(group1)]).values\n",
    "\n",
    "LR_compare(adata, train_x, train_label, subset_predict, subset_train, sparcity=sparcity, col_name='predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting purposes - Subset data to only contain prediction data with new predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make adata the predicted data with projected annotations in adata.obs[\"predicted\"]. \"corr_concat\" contains old \n",
    "# cell.labels and dataset name combined \n",
    "common_cat = \"corr_concat\"\n",
    "adata_concat = adata[:]\n",
    "adata = adata[adata.obs[\"predicted\"].isin(group1)]\n",
    "adata = adata[adata.obs[\"corr_concat\"].isin(group2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results, are displayed in a python notebook or rendered as a html\n",
    "crs_tbl = cross_table(adata, x = 'predicted', y = common_cat, highlight = True)\n",
    "crs_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot LR as a highlighted table by percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'predicted'\n",
    "y = common_cat\n",
    "\n",
    "y_attr = adata.obs[y]\n",
    "x_attr = adata.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "    e\n",
    "#Sort df columns by rows\n",
    "crs_tbl = crs_tbl.sort_values(by =list(crs_tbl.index), axis=1,ascending=False)\n",
    "\n",
    "plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, figsize=figsize, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal, vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.5})\n",
    "plt.savefig(\"./figures/LR_predictions.pdf\")\n",
    "crs_tbl.to_csv(\"./figures/pre-freq_LR_predictions_supp_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = adata.obs.groupby('predicted').count()\n",
    "prop['percentage'] = prop.iloc[:,6]/prop.iloc[:,6].sum()\n",
    "prop = prop['percentage']\n",
    "prop.to_csv(\"./figures/pre-freq_predicted_prop.csv\")\n",
    "prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redistribute the predicted values by frequency of grouping (Additive assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional clustering \n",
    "res = 5\n",
    "key_add = 'leiden'\n",
    "adata.obs[key_add] = \"nan\"\n",
    "sc.tl.leiden(adata, resolution= res, key_added= key_add, random_state=26, n_iterations=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_prediction = \"clus_prediction\"\n",
    "clusters_reassign = \"leiden\"\n",
    "lr_predicted_col = 'predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[cluster_prediction] = adata.obs.index\n",
    "for z in adata.obs[clusters_reassign].unique():\n",
    "    df = adata.obs\n",
    "    df = df[(df[clusters_reassign].isin([z]))]\n",
    "    df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "    freq_arranged = df_count.index\n",
    "    cat = freq_arranged[0]\n",
    "    df.loc[:,cluster_prediction] = cat\n",
    "    adata.obs.loc[adata.obs[clusters_reassign] == z, [cluster_prediction]] = cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Freq Redistributed proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = adata.obs.groupby('clus_prediction').count()\n",
    "prop['percentage'] = prop.iloc[:,6]/prop.iloc[:,6].sum()\n",
    "prop = prop['percentage']\n",
    "prop\n",
    "prop.to_csv(\"./figures/post-freq_predicted_leiden_consensus_prop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a concat col for old + predicted annots\n",
    "adata.obs['annot_clus_prediction_concat'] = adata.obs[cat1].astype(str) + \"_\" + adata.obs['clus_prediction'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='clus_prediction'\n",
    "y = common_cat\n",
    "\n",
    "y_attr = adata.obs[y]\n",
    "x_attr = adata.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "\n",
    "plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, figsize=figsize, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal, vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.savefig(\"./figures/LR_predictions_consensus.pdf\")\n",
    "crs_tbl.to_csv(\"./figures/post-freq_LR_predictions_consensus_supp_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = adata.obs.groupby('predicted').count()\n",
    "prop['percentage'] = prop.iloc[:,6]/prop.iloc[:,6].sum()\n",
    "prop = prop['percentage']\n",
    "prop.to_csv(\"./figures/post-freq_predicted_prop_consensus.csv\")\n",
    "prop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
